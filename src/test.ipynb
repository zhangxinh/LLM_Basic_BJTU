{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcbcf02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231266 231266\n",
      "879 879\n",
      "8549 8549\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import json\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "train_data = load_jsonl(\"../data/iwslt2017_train.jsonl\")\n",
    "val_data   = load_jsonl(\"../data/iwslt2017_validation.jsonl\")\n",
    "test_data  = load_jsonl(\"../data/iwslt2017_test.jsonl\")\n",
    "\n",
    "zh_train = [x[\"zh\"] for x in train_data]\n",
    "en_train = [x[\"en\"] for x in train_data]\n",
    "zh_val   = [x[\"zh\"] for x in val_data]\n",
    "en_val   = [x[\"en\"] for x in val_data]\n",
    "zh_test  = [x[\"zh\"] for x in test_data]\n",
    "en_test  = [x[\"en\"] for x in test_data]\n",
    "\n",
    "print(len(zh_train), len(en_train))\n",
    "print(len(zh_val), len(en_val))\n",
    "print(len(zh_test), len(en_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea489352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Vocab, count_parameters, save_model\n",
    "src_vocab = Vocab(zh_train)\n",
    "tgt_vocab = Vocab(en_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9586583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546813\n",
      "135869\n"
     ]
    }
   ],
   "source": [
    "print(len(src_vocab.stoi))\n",
    "print(len(tgt_vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "019a80f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 89\n"
     ]
    }
   ],
   "source": [
    "mx = 0\n",
    "for(s, t) in zip(zh_train, en_train):\n",
    "    mx = max(mx, len(s.split()), len(t.split()))\n",
    "print(\"Max length:\", mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f071ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint -> /opt/data/private/zxh/results/model_bs_64_all*10.pt\n",
      "Input: I like apple\n",
      "Output: prevented states, prevented states, prevented states, prevented states, prevented states, prevented states, prevented states, prevented states, prevented states, prevented states, prevented states, prevented states, prevented states, prevented states, prevented states, prevented states,\n",
      "Generated ids: [106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246, 106406, 121246]\n",
      "Top tokens:\n",
      "121246 states, 1.0000\n",
      "71116 essentially 0.0000\n",
      "131965 vicious 0.0000\n",
      "124472 tactics, 0.0000\n",
      "92710 meaning 0.0000\n",
      "98536 nutrition, 0.0000\n",
      "129014 tweet 0.0000\n",
      "66307 discipline, 0.0000\n",
      "130341 universes, 0.0000\n",
      "106083 pregnancy 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import Transformer\n",
    "from utils import load_model, BOS, EOS, PAD\n",
    "\n",
    "# 选择设备（有 CUDA 则用第一张可用 GPU，否则使用 CPU）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 根据已有的词表实例化模型（使用词表长度作为 vocab size）\n",
    "src_vocab_size = len(src_vocab.stoi)\n",
    "tgt_vocab_size = len(tgt_vocab.stoi)\n",
    "\n",
    "# 这里的超参可根据你的训练 config 调整\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model=256, N=4, h=4, d_ff=1024, dropout=0.1, max_len=128)\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint_path = \"/opt/data/private/zxh/results/model_bs_64_all*10.pt\"  # 替换为实际路径\n",
    "# 使用 utils.load_model 加载权重（map_location 指定到当前 device）\n",
    "try:\n",
    "    load_model(model, checkpoint_path, map_location=device)\n",
    "    print(f\"Loaded checkpoint -> {checkpoint_path}\")\n",
    "except Exception as e:\n",
    "    # 更宽容的回退：尝试直接用 torch.load 并处理常见 dict 包装形式\n",
    "    print(f\"load_model failed with: {e}. Trying fallback load...\")\n",
    "    state = torch.load(checkpoint_path, map_location=device)\n",
    "    if isinstance(state, dict):\n",
    "        # 常见的可能键名：'state_dict', 'model_state', 'model'\n",
    "        for k in (\"state_dict\", \"model_state\", \"model\"):\n",
    "            if k in state:\n",
    "                model.load_state_dict(state[k])\n",
    "                break\n",
    "        else:\n",
    "            # 假设直接就是 state_dict\n",
    "            model.load_state_dict(state)\n",
    "    else:\n",
    "        model.load_state_dict(state)\n",
    "    print(\"Fallback load succeeded\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 准备输入句子：使用已有的 Vocab.encode 以保证和训练时一致的格式（包含 BOS/EOS/pad）\n",
    "prompt = \"I like apple\"  # 如果你的 src 是中文请按分词方式调整\n",
    "ids = src_vocab.encode(prompt, max_len=32)\n",
    "# encode 返回已经包含 BOS/EOS 与 padding 的 id 列表\n",
    "src_tensor = torch.tensor([ids], dtype=torch.long, device=device)  # batch_size=1\n",
    "\n",
    "# 简单的贪心解码（逐步生成）——加入重复抑制与 logits 屏蔽以减少重复输出\n",
    "max_gen_len = 32\n",
    "with torch.no_grad():\n",
    "    memory = model.encode(src_tensor)\n",
    "    ys = torch.tensor([[BOS]], dtype=torch.long, device=device)  # 已有 BOS\n",
    "    prev_tok = None\n",
    "    for step in range(max_gen_len):\n",
    "        # 解码当前已生成序列\n",
    "        dec = model.decode(ys, memory)\n",
    "        logits = model.out(dec[:, -1, :])  # (batch=1, vocab)\n",
    "\n",
    "        # 屏蔽不应生成的特殊 token（例如 BOS、PAD）\n",
    "        logits[0, PAD] = -1e9\n",
    "        logits[0, BOS] = -1e9\n",
    "        # 可以在需要时屏蔽 EOS（这里保留 EOS，让模型终止）\n",
    "\n",
    "        # 简单的“禁止立即重复”策略：不允许生成与上一步相同的 token\n",
    "        if prev_tok is not None:\n",
    "            logits[0, prev_tok] = -1e9\n",
    "\n",
    "        # 取贪心\n",
    "        next_tok = logits.argmax(dim=-1).item()\n",
    "        ys = torch.cat([ys, torch.tensor([[next_tok]], dtype=torch.long, device=device)], dim=1)\n",
    "        prev_tok = next_tok\n",
    "        if next_tok == EOS:\n",
    "            break\n",
    "\n",
    "    # 将生成 id 列表（去掉 BOS/EOS/PAD）转换为 token 并合并为字符串输出\n",
    "    gen_ids = ys.squeeze(0).tolist()\n",
    "    # 去掉开头 BOS\n",
    "    if gen_ids and gen_ids[0] == BOS:\n",
    "        gen_ids = gen_ids[1:]\n",
    "    # 截断到 EOS\n",
    "    if EOS in gen_ids:\n",
    "        gen_ids = gen_ids[:gen_ids.index(EOS)]\n",
    "    # 去掉 PAD\n",
    "    gen_ids = [i for i in gen_ids if i != PAD]\n",
    "\n",
    "    output_tokens = [tgt_vocab.itos.get(i, \"<unk>\") for i in gen_ids]\n",
    "    output = \" \".join(output_tokens)\n",
    "\n",
    "print(\"Input:\", prompt)\n",
    "print(\"Output:\", output)\n",
    "\n",
    "# 诊断信息，帮助判断是否为 vocab/加载问题\n",
    "print(\"Generated ids:\", gen_ids)\n",
    "# 打印前 20 logits topk 作快速检查（最后一步）\n",
    "try:\n",
    "    topk = torch.topk(torch.softmax(logits, dim=-1), k=10)\n",
    "    print(\"Top tokens:\")\n",
    "    for score, idx in zip(topk.values.squeeze(0).tolist(), topk.indices.squeeze(0).tolist()):\n",
    "        print(idx, tgt_vocab.itos.get(idx, \"<unk>\"), f\"{score:.4f}\")\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf70136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_none_ps_loss.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_path = \"results/model_none_ps.pt\"\n",
    "\n",
    "# 获取文件名（不含路径）\n",
    "filename = os.path.basename(save_path)\n",
    "# 去掉扩展名并加上 _loss.png\n",
    "loss_path = os.path.splitext(filename)[0] + \"_loss.png\"\n",
    "\n",
    "print(loss_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
